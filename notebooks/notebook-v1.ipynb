{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m77 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m71 packages\u001b[0m \u001b[2min 0.08ms\u001b[0m\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m77 packages\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m71 packages\u001b[0m \u001b[2min 0.06ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add pandas numpy lightgbm duckdb ipywidgets --active\n",
    "!uv sync --active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports e Configuração Inicial  \n",
    "Primeiro, vamos importar todas as bibliotecas que precisaremos. Estamos usando uma stack bem robusta: pandas para manipulação de dados, DuckDB para consultas SQL rápidas, LightGBM para modelagem, e algumas outras ferramentas úteis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classe Principal - PrevisaoVendasHackathon\n",
    "Esta é a classe que faz toda a mágica acontecer. Ela encapsula todo o pipeline de machine learning, desde o carregamento dos dados até a geração das previsões finais.\n",
    "A classe tem vários atributos importantes: conexão com DuckDB para processar dados grandes de forma eficiente, o modelo LightGBM treinado, lista de features selecionadas, encoders para variáveis categóricas, e resultados de validação.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrevisaoVendasHackathon:\n",
    "    def __init__(self, verbose=True):\n",
    "        self.verbose = verbose\n",
    "        self.conn = None\n",
    "        self.model = None\n",
    "        self.feature_cols = []\n",
    "        self.label_encoders = {}\n",
    "        self.validation_results = {}\n",
    "        self.all_combinations = None\n",
    "        \n",
    "    def inicializar_duckdb(self, memory_limit='12GB', threads=10):\n",
    "        \"\"\"Inicializa DuckDB com configurações otimizadas\"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"Inicializando DuckDB para previsão do hackathon...\")\n",
    "        \n",
    "        self.conn = duckdb.connect(':memory:')\n",
    "        self.conn.execute(f\"SET memory_limit='{memory_limit}'\")\n",
    "        self.conn.execute(f\"SET threads={threads}\")\n",
    "\n",
    "        self.conn.execute(\"SET preserve_insertion_order=false\")\n",
    "        self.conn.execute(\"SET temp_directory='/tmp'\")  # Usa disco para overflow\n",
    "    \n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"   Memória: {memory_limit}, Threads: {threads}\")\n",
    "    \n",
    "    def carregar_e_juntar_dados(self, arquivo_loja, arquivo_transacao, arquivo_produto):\n",
    "        \"\"\"Carrega todos os conjuntos de dados e cria visão principal com joins adequados - CORRIGIDO para IDs grandes\"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"Carregando e juntando conjuntos de dados...\")\n",
    "        \n",
    "        # Carrega tabelas individuais - USANDO BIGINT para IDs grandes\n",
    "        self.conn.execute(f\"\"\"\n",
    "            CREATE VIEW stores AS \n",
    "            SELECT CAST(pdv AS BIGINT) as pdv, \n",
    "                   premise, \n",
    "                   categoria_pdv, \n",
    "                   CAST(zipcode AS BIGINT) as zipcode \n",
    "            FROM '{arquivo_loja}'\n",
    "        \"\"\")\n",
    "        \n",
    "        self.conn.execute(f\"\"\"\n",
    "            CREATE VIEW transactions AS \n",
    "            SELECT CAST(internal_store_id AS BIGINT) as internal_store_id, \n",
    "                   CAST(internal_product_id AS BIGINT) as internal_product_id, \n",
    "                   transaction_date,\n",
    "                   CAST(quantity AS INTEGER) as quantity,\n",
    "                   net_value,\n",
    "                   gross_value\n",
    "            FROM '{arquivo_transacao}'\n",
    "        \"\"\")\n",
    "        \n",
    "        self.conn.execute(f\"\"\"\n",
    "            CREATE VIEW products AS \n",
    "            SELECT CAST(produto AS BIGINT) as produto, \n",
    "                   categoria, \n",
    "                   tipos, \n",
    "                   label, \n",
    "                   subcategoria \n",
    "            FROM '{arquivo_produto}'\n",
    "        \"\"\")\n",
    "        \n",
    "        # Cria conjunto de dados principal com joins - focando nos dados de 2022\n",
    "        master_query = \"\"\"\n",
    "        CREATE VIEW master_data AS\n",
    "        SELECT \n",
    "            t.internal_store_id as pdv,\n",
    "            t.internal_product_id as produto,\n",
    "            t.transaction_date,\n",
    "            t.quantity,\n",
    "            COALESCE(t.net_value, 0) as net_value,\n",
    "            COALESCE(t.gross_value, 0) as gross_value,\n",
    "            -- Atributos da loja\n",
    "            COALESCE(s.premise, 'Unknown') as premise,\n",
    "            COALESCE(s.categoria_pdv, 'Unknown') as categoria_pdv,\n",
    "            COALESCE(s.zipcode, 0) as zipcode,\n",
    "            -- Atributos do produto\n",
    "            COALESCE(p.categoria, 'Unknown') as product_categoria,\n",
    "            COALESCE(p.tipos, 'Unknown') as product_type,\n",
    "            COALESCE(p.label, 'Unknown') as product_label,\n",
    "            COALESCE(p.subcategoria, 'Unknown') as subcategoria,\n",
    "            -- Características de tempo\n",
    "            EXTRACT('year' FROM t.transaction_date) as year,\n",
    "            EXTRACT('week' FROM t.transaction_date) as week_number,\n",
    "            EXTRACT('month' FROM t.transaction_date) as month,\n",
    "            EXTRACT('quarter' FROM t.transaction_date) as quarter,\n",
    "            EXTRACT('dayofweek' FROM t.transaction_date) as day_of_week\n",
    "        FROM transactions t\n",
    "        LEFT JOIN stores s ON t.internal_store_id = s.pdv\n",
    "        LEFT JOIN products p ON t.internal_product_id = p.produto\n",
    "        WHERE t.quantity > 0 \n",
    "          AND t.quantity IS NOT NULL\n",
    "          AND t.transaction_date IS NOT NULL\n",
    "          AND EXTRACT('year' FROM t.transaction_date) = 2022\n",
    "        \"\"\"\n",
    "        \n",
    "        self.conn.execute(master_query)\n",
    "        \n",
    "        # Obtém todas as combinações únicas PDV-Produto para previsão\n",
    "        self.conn.execute(\"\"\"\n",
    "            CREATE VIEW all_combinations AS\n",
    "            SELECT DISTINCT pdv, produto\n",
    "            FROM master_data\n",
    "        \"\"\")\n",
    "        \n",
    "        # Obtém estatísticas de qualidade dos dados\n",
    "        stats = self.conn.execute(\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total_records,\n",
    "                COUNT(DISTINCT pdv) as unique_pdvs,\n",
    "                COUNT(DISTINCT produto) as unique_produtos,\n",
    "                COUNT(DISTINCT CAST(pdv AS VARCHAR) || '_' || CAST(produto AS VARCHAR)) as unique_combinations,\n",
    "                MIN(transaction_date) as min_date,\n",
    "                MAX(transaction_date) as max_date,\n",
    "                SUM(quantity) as total_quantity,\n",
    "                AVG(quantity) as avg_quantity\n",
    "            FROM master_data\n",
    "        \"\"\").fetchone()\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"   Registros: {stats[0]:,}\")\n",
    "            print(f\"   PDVs: {stats[1]:,}\")\n",
    "            print(f\"   Produtos: {stats[2]:,}\")\n",
    "            print(f\"   Combinações PDV-Produto: {stats[3]:,}\")\n",
    "            print(f\"   Período: {stats[4]} a {stats[5]}\")\n",
    "    \n",
    "    def criar_agregacao_semanal(self):\n",
    "        \"\"\"Cria agregação semanal para todas as combinações PDV-Produto\"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"Criando agregação semanal...\")\n",
    "        \n",
    "        # Agregação semanal simplificada \n",
    "        weekly_agg_query = \"\"\"\n",
    "        CREATE VIEW weekly_data AS\n",
    "        SELECT \n",
    "            m.pdv,\n",
    "            m.produto,\n",
    "            m.year,\n",
    "            m.week_number,\n",
    "            -- Variável objetivo\n",
    "            SUM(m.quantity) as quantity,\n",
    "            -- Métricas de transação\n",
    "            COUNT(*) as transaction_count,\n",
    "            COUNT(DISTINCT m.transaction_date) as unique_days,\n",
    "            SUM(m.net_value) as net_value,\n",
    "            CASE WHEN SUM(m.quantity) > 0 \n",
    "                 THEN SUM(m.net_value) / SUM(m.quantity) \n",
    "                 ELSE 0 END as avg_unit_price,\n",
    "            -- Características de contexto - usando FIRST() em vez de agregações complexas\n",
    "            FIRST(m.premise) as premise,\n",
    "            FIRST(m.categoria_pdv) as categoria_pdv,\n",
    "            FIRST(m.product_categoria) as product_categoria,\n",
    "            FIRST(m.product_type) as product_type,\n",
    "            FIRST(m.product_label) as product_label,\n",
    "            FIRST(m.subcategoria) as subcategoria,\n",
    "            FIRST(m.zipcode) as zipcode,\n",
    "            -- Características de tempo\n",
    "            m.month,\n",
    "            m.quarter\n",
    "        FROM master_data m\n",
    "        GROUP BY m.pdv, m.produto, m.year, m.week_number, m.month, m.quarter\n",
    "        ORDER BY pdv, produto, year, week_number\n",
    "        \"\"\"\n",
    "        \n",
    "        self.conn.execute(weekly_agg_query)\n",
    "        \n",
    "        # Estatísticas\n",
    "        agg_stats = self.conn.execute(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as weekly_records,\n",
    "            COUNT(DISTINCT pdv) as unique_pdvs,\n",
    "            COUNT(DISTINCT produto) as unique_produtos,\n",
    "            MIN(week_number) as min_week,\n",
    "            MAX(week_number) as max_week,\n",
    "            SUM(quantity) as total_quantity\n",
    "        FROM weekly_data\n",
    "        \"\"\").fetchone()\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"   Registros semanais: {agg_stats[0]:,}\")\n",
    "            print(f\"   Intervalo de semanas: {agg_stats[3]}-{agg_stats[4]}\")\n",
    "            print(f\"   Quantidade total: {agg_stats[5]:,}\")\n",
    "    \n",
    "    def criar_caracteristicas_ml(self):\n",
    "        \"\"\"Cria características abrangentes de ML usando funções de janela\"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"Criando características de ML...\")\n",
    "        \n",
    "        features_query = \"\"\"\n",
    "        CREATE VIEW ml_features AS\n",
    "        SELECT \n",
    "            *,\n",
    "            -- Características de sazonalidade (codificação cíclica)\n",
    "            SIN(2 * PI() * week_number / 52.0) as week_sin,\n",
    "            COS(2 * PI() * week_number / 52.0) as week_cos,\n",
    "            SIN(2 * PI() * month / 12.0) as month_sin,\n",
    "            COS(2 * PI() * month / 12.0) as month_cos,\n",
    "            \n",
    "            -- Características de atraso por PDV-Produto\n",
    "            LAG(quantity, 1) OVER pdv_produto_window as lag_1,\n",
    "            LAG(quantity, 2) OVER pdv_produto_window as lag_2,\n",
    "            LAG(quantity, 4) OVER pdv_produto_window as lag_4,\n",
    "            \n",
    "            -- Estatísticas móveis (simplificadas)\n",
    "            AVG(quantity) OVER (PARTITION BY pdv, produto ORDER BY year, week_number \n",
    "                               ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING) as rolling_mean_4,\n",
    "            AVG(quantity) OVER (PARTITION BY pdv, produto ORDER BY year, week_number \n",
    "                               ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING) as rolling_mean_8,\n",
    "            \n",
    "            -- Características transversais (simplificadas)\n",
    "            AVG(quantity) OVER produto_window as produto_avg_quantity,\n",
    "            AVG(quantity) OVER pdv_window as pdv_avg_quantity,\n",
    "            \n",
    "            -- Características de preço e momentum\n",
    "            COALESCE(avg_unit_price, 0) as unit_price,\n",
    "            LAG(COALESCE(avg_unit_price, 0), 1) OVER pdv_produto_window as price_lag_1,\n",
    "            COALESCE(LAG(quantity, 1) OVER pdv_produto_window, 0) - \n",
    "            COALESCE(LAG(quantity, 2) OVER pdv_produto_window, 0) as momentum_1,\n",
    "            \n",
    "            -- Intensidade de transação\n",
    "            COALESCE(transaction_count, 1) * COALESCE(avg_unit_price, 0) as transaction_value,\n",
    "            COALESCE(unique_days, 1) / 7.0 as days_active_ratio\n",
    "            \n",
    "        FROM weekly_data\n",
    "        WINDOW \n",
    "            pdv_produto_window AS (PARTITION BY pdv, produto ORDER BY year, week_number),\n",
    "            produto_window AS (PARTITION BY produto),\n",
    "            pdv_window AS (PARTITION BY pdv)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.conn.execute(features_query)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"   Engenharia de características completa\")\n",
    "    \n",
    "    def preparar_dados_treinamento(self):\n",
    "        if self.verbose:\n",
    "            print(\"Preparando dados de treinamento...\")\n",
    "        \n",
    "        # Extrai dados do DuckDB\n",
    "        ml_data = self.conn.execute(\"SELECT * FROM ml_features ORDER BY pdv, produto, year, week_number\").fetchdf()\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"   Extraídos {len(ml_data):,} registros\")\n",
    "        \n",
    "        categorical_cols = ['premise', 'categoria_pdv', 'product_categoria', \n",
    "                           'product_type', 'product_label', 'subcategoria']\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            if col in ml_data.columns:\n",
    "                le = LabelEncoder()\n",
    "                valid_mask = ml_data[col].notna()\n",
    "                if valid_mask.sum() > 0:\n",
    "                    ml_data.loc[valid_mask, f'{col}_encoded'] = le.fit_transform(\n",
    "                        ml_data.loc[valid_mask, col].astype(str)\n",
    "                    )\n",
    "                    ml_data[f'{col}_encoded'] = ml_data[f'{col}_encoded'].fillna(-1)\n",
    "                    self.label_encoders[col] = le\n",
    "        \n",
    "        # Seleciona características para modelagem (exclui alvo e identificadores)\n",
    "        exclude_cols = ['quantity', 'pdv', 'produto', 'year', 'week_number', 'net_value',\n",
    "                       'premise', 'categoria_pdv', 'product_categoria', 'product_type', \n",
    "                       'product_label', 'subcategoria', 'month', 'quarter']\n",
    "        \n",
    "        self.feature_cols = []\n",
    "        for col in ml_data.columns:\n",
    "            if (col not in exclude_cols and \n",
    "                ml_data[col].dtype in ['int64', 'float64', 'int32', 'float32'] and\n",
    "                not ml_data[col].isna().all()):\n",
    "                self.feature_cols.append(col)\n",
    "        \n",
    "        # Limpa conjunto de dados\n",
    "        for col in self.feature_cols:\n",
    "            ml_data[col] = ml_data[col].fillna(0)\n",
    "        \n",
    "        clean_data = ml_data.dropna(subset=['quantity']).copy()\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"   Características selecionadas: {len(self.feature_cols)}\")\n",
    "            print(f\"   Registros limpos: {len(clean_data):,}\")\n",
    "        \n",
    "        return clean_data\n",
    "    \n",
    "    def divisao_validacao_temporal(self, data):\n",
    "        \"\"\"Cria divisão de validação temporal - últimas 4 semanas para validação\"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"Criando divisão de validação temporal...\")\n",
    "        \n",
    "        # Usa as últimas 4 semanas de 2022 para validação (semanas 49-52)\n",
    "        train_data = data[data['week_number'] <= 48].copy()\n",
    "        val_data = data[data['week_number'] >= 49].copy()\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"   Treinamento: {len(train_data):,} registros (semanas 1-48)\")\n",
    "            print(f\"   Validação: {len(val_data):,} registros (semanas 49-52)\")\n",
    "        \n",
    "        return train_data, val_data\n",
    "\n",
    "    def treinar_modelo_lightgbm(self, train_data, val_data=None):\n",
    "        \"\"\"Treina modelo LightGBM otimizado para previsão de vendas\"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"Treinando modelo LightGBM...\")\n",
    "\n",
    "        X_train = train_data[self.feature_cols]\n",
    "        y_train = train_data['quantity']\n",
    "\n",
    "        # Configuração do LightGBM otimizada para previsão de vendas\n",
    "        lgb_params = {\n",
    "            'objective': 'poisson',\n",
    "            'metric': 'poisson',\n",
    "            'num_leaves': 127,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 1,\n",
    "            'min_data_in_leaf': 20,\n",
    "            'n_estimators': 1000,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1,\n",
    "        }\n",
    "\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            self.model = lgb.LGBMRegressor(**lgb_params)\n",
    "\n",
    "            callbacks = []\n",
    "            if val_data is not None:\n",
    "                X_val = val_data[self.feature_cols]\n",
    "                y_val = val_data['quantity']\n",
    "                \n",
    "                callbacks.append(lgb.early_stopping(stopping_rounds=50))\n",
    "                \n",
    "                # Log do progresso\n",
    "                if self.verbose:\n",
    "                    callbacks.append(lgb.log_evaluation(period=100))\n",
    "\n",
    "                self.model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_val, y_val)],\n",
    "                    eval_metric='poisson',\n",
    "                    callbacks=callbacks\n",
    "                )\n",
    "            else:\n",
    "                # Treina sem parada antecipada\n",
    "                self.model.fit(X_train, y_train)\n",
    "\n",
    "            train_time = time.time() - start_time\n",
    "            if self.verbose:\n",
    "                print(f\"   Treinamento concluído em {train_time:.1f}s\")\n",
    "\n",
    "        except Exception as e:\n",
    "            if self.verbose:\n",
    "                print(f\"   Erro no treinamento: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def validar_modelo(self, val_data):\n",
    "        if self.verbose:\n",
    "            print(\"Validando modelo...\")\n",
    "        \n",
    "        X_val = val_data[self.feature_cols]\n",
    "        y_val = val_data['quantity']\n",
    "        \n",
    "        pred = self.model.predict(X_val)\n",
    "        pred = np.maximum(0, pred)  # Garante não-negatividade\n",
    "        \n",
    "        # Calcula WMAPE\n",
    "        wmape = np.sum(np.abs(y_val - pred)) / np.sum(np.abs(y_val)) if np.sum(np.abs(y_val)) > 0 else 0\n",
    "        mae = mean_absolute_error(y_val, pred)\n",
    "        \n",
    "        self.validation_results = {\n",
    "            'wmape': wmape,\n",
    "            'mae': mae,\n",
    "            'predictions': pred\n",
    "        }\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"   WMAPE = {wmape:.4f} ({wmape*100:.2f}%)\")\n",
    "            print(f\"   MAE = {mae:.2f}\")\n",
    "        \n",
    "        return self.validation_results\n",
    "    \n",
    "    def gerar_previsao_janeiro_2023(self, trained_data):\n",
    "        if self.verbose:\n",
    "            print(\"Gerando previsões para Janeiro 2023 (semanas 1-5)...\")\n",
    "        \n",
    "        last_states_query = \"\"\"\n",
    "        WITH ranked_data AS (\n",
    "            SELECT *,\n",
    "                ROW_NUMBER() OVER (PARTITION BY pdv, produto ORDER BY year DESC, week_number DESC) as rn\n",
    "            FROM ml_features\n",
    "        ),\n",
    "        latest_data AS (\n",
    "            SELECT \n",
    "                pdv, produto, premise, categoria_pdv, product_categoria,\n",
    "                product_type, product_label, subcategoria, zipcode,\n",
    "                quantity as last_quantity,\n",
    "                avg_unit_price as last_price,\n",
    "                rolling_mean_4 as last_rolling_mean_4,\n",
    "                rolling_mean_8 as last_rolling_mean_8,\n",
    "                produto_avg_quantity,\n",
    "                pdv_avg_quantity,\n",
    "                transaction_count,\n",
    "                unique_days\n",
    "            FROM ranked_data \n",
    "            WHERE rn = 1\n",
    "        )\n",
    "        SELECT * FROM latest_data\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"   Extraindo estados mais recentes para todas as combinações PDV-Produto...\")\n",
    "        \n",
    "        last_states = self.conn.execute(last_states_query).fetchdf()\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"   Encontradas {len(last_states):,} combinações únicas PDV-Produto\")\n",
    "        \n",
    "        # Adiciona codificações categóricas aos last_states\n",
    "        categorical_cols = [\n",
    "            'premise', 'categoria_pdv', 'product_categoria',\n",
    "            'product_type', 'product_label', 'subcategoria'\n",
    "        ]\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            if col in last_states.columns and col in self.label_encoders:\n",
    "                le = self.label_encoders[col]\n",
    "                encoded_col = f'{col}_encoded'\n",
    "                try:\n",
    "                    # Trata categorias não vistas com elegância\n",
    "                    encoded_values = []\n",
    "                    for val in last_states[col].astype(str):\n",
    "                        try:\n",
    "                            encoded_values.append(le.transform([val])[0])\n",
    "                        except ValueError:\n",
    "                            encoded_values.append(-1)  # Categoria desconhecida\n",
    "                    last_states[encoded_col] = encoded_values\n",
    "                except Exception as e:\n",
    "                    if self.verbose:\n",
    "                        print(f\"   Aviso na codificação {col}: {e}\")\n",
    "                    last_states[encoded_col] = -1\n",
    "        \n",
    "        target_weeks = [1, 2, 3, 4, 5]  # Semanas de Janeiro 2023\n",
    "        forecasts = []\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"   Criando características de previsão para semanas {target_weeks}...\")\n",
    "        \n",
    "        # Pré-calcula características de tempo para todas as semanas\n",
    "        time_features = {}\n",
    "        for week in target_weeks:\n",
    "            time_features[week] = {\n",
    "                'week_sin': np.sin(2 * np.pi * week / 52),\n",
    "                'week_cos': np.cos(2 * np.pi * week / 52),\n",
    "                'month_sin': np.sin(2 * np.pi * 1 / 12),  # Janeiro\n",
    "                'month_cos': np.cos(2 * np.pi * 1 / 12)\n",
    "            }\n",
    "        \n",
    "        # Cria matriz de características para todas as combinações e semanas de uma vez\n",
    "        total_predictions = len(last_states) * len(target_weeks)\n",
    "        if self.verbose:\n",
    "            print(f\"   Preparando {total_predictions:,} previsões...\")\n",
    "        \n",
    "        # Processa em lotes para evitar problemas de memória\n",
    "        batch_size = 500000 \n",
    "        n_batches = (len(last_states) + batch_size - 1) // batch_size\n",
    "        \n",
    "        for batch_idx in range(n_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min((batch_idx + 1) * batch_size, len(last_states))\n",
    "            batch_states = last_states.iloc[start_idx:end_idx]\n",
    "            \n",
    "            if self.verbose and n_batches > 1:\n",
    "                print(f\"   Processando lote {batch_idx + 1}/{n_batches} ({len(batch_states):,} combinações)\")\n",
    "            \n",
    "            batch_predictions = []\n",
    "            \n",
    "            for week in target_weeks:\n",
    "                week_features = []\n",
    "                \n",
    "                for _, row in batch_states.iterrows():\n",
    "                    try:\n",
    "                        # Cria vetor de características\n",
    "                        feature_dict = {}\n",
    "                        \n",
    "                        # Características de tempo\n",
    "                        feature_dict.update(time_features[week])\n",
    "                        \n",
    "                        # Características de atraso (usa últimos valores conhecidos)\n",
    "                        feature_dict['lag_1'] = row.get('last_quantity', 0)\n",
    "                        feature_dict['lag_2'] = row.get('last_quantity', 0)\n",
    "                        feature_dict['lag_4'] = row.get('last_quantity', 0)\n",
    "                        \n",
    "                        # Estatísticas móveis\n",
    "                        feature_dict['rolling_mean_4'] = row.get('last_rolling_mean_4', 0)\n",
    "                        feature_dict['rolling_mean_8'] = row.get('last_rolling_mean_8', 0)\n",
    "                        \n",
    "                        # Características transversais\n",
    "                        feature_dict['produto_avg_quantity'] = row.get('produto_avg_quantity', 0)\n",
    "                        feature_dict['pdv_avg_quantity'] = row.get('pdv_avg_quantity', 0)\n",
    "                        \n",
    "                        # Outras características\n",
    "                        feature_dict['unit_price'] = row.get('last_price', 0)\n",
    "                        feature_dict['price_lag_1'] = row.get('last_price', 0)\n",
    "                        feature_dict['momentum_1'] = 0\n",
    "                        feature_dict['transaction_value'] = row.get('transaction_count', 1) * row.get('last_price', 0)\n",
    "                        feature_dict['days_active_ratio'] = row.get('unique_days', 1) / 7.0\n",
    "                        feature_dict['zipcode'] = row.get('zipcode', 0)\n",
    "                        feature_dict['transaction_count'] = row.get('transaction_count', 1)\n",
    "                        feature_dict['unique_days'] = row.get('unique_days', 1)\n",
    "                        \n",
    "                        # Adiciona características categóricas codificadas\n",
    "                        for col in categorical_cols:\n",
    "                            encoded_col = f'{col}_encoded'\n",
    "                            feature_dict[encoded_col] = row.get(encoded_col, -1)\n",
    "                        \n",
    "                        # Cria vetor de características na ordem correta\n",
    "                        feature_vector = [feature_dict.get(col, 0) for col in self.feature_cols]\n",
    "                        week_features.append((row['pdv'], row['produto'], week, feature_vector))\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        if self.verbose:\n",
    "                            print(f\"   Erro ao criar características para PDV {row['pdv']}, Produto {row['produto']}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # Faz previsões em lote para esta semana\n",
    "                if week_features:\n",
    "                    try:\n",
    "                        X_batch = pd.DataFrame([f[3] for f in week_features], columns=self.feature_cols).fillna(0)\n",
    "                        predictions = self.model.predict(X_batch)\n",
    "                        \n",
    "                        # Armazena resultados\n",
    "                        for i, (pdv, produto, semana, _) in enumerate(week_features):\n",
    "                            pred = max(0, round(predictions[i]))\n",
    "                            batch_predictions.append({\n",
    "                                'semana': int(semana),\n",
    "                                'pdv': int(pdv),\n",
    "                                'produto': int(produto),\n",
    "                                'quantidade': int(pred)\n",
    "                            })\n",
    "                    except Exception as e:\n",
    "                        if self.verbose:\n",
    "                            print(f\"   Erro na previsão em lote para semana {week}: {e}\")\n",
    "                        continue\n",
    "            \n",
    "            forecasts.extend(batch_predictions)\n",
    "            \n",
    "            # Atualização de progresso\n",
    "            if self.verbose and len(forecasts) > 0:\n",
    "                progress = (batch_idx + 1) / n_batches * 100\n",
    "                print(f\"   Progresso: {progress:.1f}% ({len(forecasts):,} previsões geradas)\")\n",
    "        \n",
    "        forecast_df = pd.DataFrame(forecasts)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"   Geradas {len(forecast_df):,} previsões\")\n",
    "            if len(forecast_df) > 0:\n",
    "                print(f\"   PDVs únicos: {forecast_df['pdv'].nunique():,}\")\n",
    "                print(f\"   Produtos únicos: {forecast_df['produto'].nunique():,}\")\n",
    "                print(f\"   Quantidade média: {forecast_df['quantidade'].mean():.1f}\")\n",
    "                \n",
    "                # Resumo semanal\n",
    "                weekly_summary = forecast_df.groupby('semana')['quantidade'].agg(['count', 'sum', 'mean']).round(1)\n",
    "                print(\"   Resumo semanal:\")\n",
    "                for week, (count, total, avg) in weekly_summary.iterrows():\n",
    "                    print(f\"      Semana {week}: {int(count):,} previsões, {int(total):,} qtd total, {avg:.1f} qtd média\")\n",
    "        \n",
    "        return forecast_df\n",
    "        \n",
    "    def salvar_formato_hackathon(self, forecast_df, output_path):\n",
    "        \"\"\"Salva previsão no formato exato do hackathon\"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"Salvando previsão no formato do hackathon...\")\n",
    "        \n",
    "        hackathon_df = forecast_df[['semana', 'pdv', 'produto', 'quantidade']].copy()\n",
    "        \n",
    "        # Salva com separador ponto e vírgula e codificação UTF-8\n",
    "        hackathon_df.to_csv(\n",
    "            output_path, \n",
    "            index=False, \n",
    "            sep=';',           \n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"   Salvo em: {output_path}\")\n",
    "            print(f\"   Formato: CSV com separador ';', codificação UTF-8\")\n",
    "            print(f\"   Registros: {len(hackathon_df):,}\")\n",
    "    \n",
    "    def executar_pipeline_completo(self, arquivo_loja, arquivo_transacao, arquivo_produto, \n",
    "                             output_path=\"hackathon_v1.csv\"):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        print(\"PIPELINE DE PREVISÃO DE VENDAS - HACKATHON\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        try:\n",
    "            # 1. Inicializa DuckDB\n",
    "            self.inicializar_duckdb()\n",
    "            \n",
    "            # 2. Carrega e junta dados\n",
    "            self.carregar_e_juntar_dados(arquivo_loja, arquivo_transacao, arquivo_produto)\n",
    "            \n",
    "            # 3. Cria agregação semanal\n",
    "            self.criar_agregacao_semanal()\n",
    "            \n",
    "            # 4. Cria características de ML\n",
    "            self.criar_caracteristicas_ml()\n",
    "            \n",
    "            # 5. Prepara dados de treinamento\n",
    "            training_data = self.preparar_dados_treinamento()\n",
    "            \n",
    "            # 6. Cria divisão temporal para validação\n",
    "            train_data, val_data = self.divisao_validacao_temporal(training_data)\n",
    "            \n",
    "            # 7. Treina modelo com validação\n",
    "            self.treinar_modelo_lightgbm(train_data, val_data)\n",
    "            \n",
    "            # 8. Valida modelo\n",
    "            validation_results = self.validar_modelo(val_data)\n",
    "            \n",
    "            # 9. Retreina com dados completos de 2022 para previsão final (sem parada antecipada)\n",
    "            if self.verbose:\n",
    "                print(\"Retreinando com conjunto completo de dados de 2022...\")\n",
    "            self.treinar_modelo_lightgbm(training_data)\n",
    "            \n",
    "            # 10. Gera previsões para Janeiro 2023 (5 semanas para TODAS as combinações)\n",
    "            forecast_df = self.gerar_previsao_janeiro_2023(training_data)\n",
    "            \n",
    "            # 11. Salva no formato do hackathon\n",
    "            self.salvar_formato_hackathon(forecast_df, output_path)\n",
    "            \n",
    "            # 12. Limpeza\n",
    "            self.conn.close()\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"Tempo total: {total_time:.1f}s ({total_time/60:.1f}min)\")\n",
    "            print(f\"Previsões: {len(forecast_df):,}\")\n",
    "            print(f\"Saída: {output_path}\")\n",
    "            \n",
    "            # Mostra resultados da validação\n",
    "            if validation_results:\n",
    "                print(f\"\\nRESULTADOS DA VALIDAÇÃO (semanas 49-52 de 2022):\")\n",
    "                wmape_pct = validation_results['wmape'] * 100\n",
    "                print(f\"   LightGBM: {wmape_pct:.2f}% WMAPE\")\n",
    "                print(f\"   MAE: {validation_results['mae']:.2f}\")\n",
    "            \n",
    "            return forecast_df, validation_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erro no pipeline: {e}\")\n",
    "            if self.conn:\n",
    "                self.conn.close()\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Função Principal de Execução  \n",
    "Esta função orquestra todo o processo: carrega os dados, treina o modelo, valida os resultados e gera as previsões para Janeiro 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executar_previsao_hackathon(arquivo_loja, arquivo_transacao, arquivo_produto, arquivo_saida=\"hackathon_v1.csv\"):\n",
    "    print(\"PREVISÃO DE VENDAS - HACKATHON\")\n",
    "    \n",
    "    hackathon_pipeline = PrevisaoVendasHackathon(verbose=True)\n",
    "    \n",
    "    forecast_result, validation_metrics = hackathon_pipeline.executar_pipeline_completo(\n",
    "        arquivo_loja=arquivo_loja,\n",
    "        arquivo_transacao=arquivo_transacao,\n",
    "        arquivo_produto=arquivo_produto,\n",
    "        output_path=arquivo_saida\n",
    "    )\n",
    "    \n",
    "    if forecast_result is not None:\n",
    "        print(\"\\nSUBMISSÃO DO HACKATHON PRONTA!\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Arquivo: {arquivo_saida}\")\n",
    "        print(\"Formato: semana;pdv;produto;quantidade\")\n",
    "        print(\"Codificação: UTF-8\")\n",
    "        print(\"Semanas: 1-5 (Janeiro 2023)\")\n",
    "        \n",
    "        print(\"\\nAMOSTRA DAS PREVISÕES:\")\n",
    "        print(forecast_result.head(10).to_string(index=False))\n",
    "        \n",
    "        print(\"\\nRESUMO SEMANAL:\")\n",
    "        weekly_summary = forecast_result.groupby('semana').agg({\n",
    "            'quantidade': ['count', 'sum', 'mean']\n",
    "        }).round(1)\n",
    "        weekly_summary.columns = ['Previsões', 'Qtd_Total', 'Qtd_Média']\n",
    "        print(weekly_summary.to_string())\n",
    "        \n",
    "        # Verificações finais de qualidade\n",
    "        print(\"\\nVERIFICAÇÕES DE QUALIDADE:\")\n",
    "        print(f\"   Total de previsões: {len(forecast_result):,}\")\n",
    "        print(f\"   PDVs únicos: {forecast_result['pdv'].nunique():,}\")\n",
    "        print(f\"   Produtos únicos: {forecast_result['produto'].nunique():,}\")\n",
    "        print(f\"   Semanas cobertas: {sorted(forecast_result['semana'].unique())}\")\n",
    "        print(f\"   Tipos de dados: Todos inteiros = {all(forecast_result.dtypes == 'int64')}\")\n",
    "        print(f\"   Valores faltantes: {forecast_result.isnull().sum().sum()}\")\n",
    "        print(f\"   Quantidades negativas: {(forecast_result['quantidade'] < 0).sum()}\")\n",
    "        \n",
    "        if os.path.exists(arquivo_saida):\n",
    "            file_size = os.path.getsize(arquivo_saida) / 1024 / 1024 \n",
    "            print(f\"   Tamanho do arquivo: {file_size:.1f} MB\")\n",
    "        \n",
    "        print(\"\\nPronto para submissão no hackathon!\")\n",
    "        return forecast_result, validation_metrics\n",
    "        \n",
    "    else:\n",
    "        print(\"Pipeline falhou. Verifique seus arquivos de dados e caminhos.\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções Utilitárias de Validação\n",
    "A função principal aqui verifica se sua submissão está no formato certinho: separador correto, encoding UTF-8, tipos de dados adequados, sem valores negativos ou faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validar_saida_hackathon(csv_path=\"hackathon_v1.csv\"):\n",
    "    try:\n",
    "        # Lê o CSV com separador ponto e vírgula\n",
    "        df = pd.read_csv(csv_path, sep=';', encoding='utf-8')\n",
    "        \n",
    "        print(f\"Arquivo carregado com sucesso\")\n",
    "        print(f\"Forma: {df.shape}\")\n",
    "        \n",
    "        # Verifica nomes das colunas\n",
    "        expected_cols = ['semana', 'pdv', 'produto', 'quantidade']\n",
    "        actual_cols = list(df.columns)\n",
    "        \n",
    "        if actual_cols == expected_cols:\n",
    "            print(f\"Nomes das colunas corretos: {actual_cols}\")\n",
    "        else:\n",
    "            print(f\"Nomes das colunas incorretos!\")\n",
    "            print(f\"   Esperado: {expected_cols}\")\n",
    "            print(f\"   Atual: {actual_cols}\")\n",
    "            return False\n",
    "        \n",
    "        # Verifica tipos de dados (devem ser todos inteiros, mas permite IDs grandes)\n",
    "        print(f\"Tipos de dados:\")\n",
    "        for col in df.columns:\n",
    "            dtype = df[col].dtype\n",
    "            is_integer = pd.api.types.is_integer_dtype(dtype)\n",
    "            status = \"✓\" if is_integer else \"✗\"\n",
    "            print(f\"   {status} {col}: {dtype}\")\n",
    "            \n",
    "            # Verifica IDs grandes\n",
    "            if col in ['pdv', 'produto'] and is_integer:\n",
    "                max_val = df[col].max()\n",
    "                if max_val > 2**31:\n",
    "                    print(f\"      ID grande detectado (máx: {max_val}) - usando 64-bit\")\n",
    "        \n",
    "        # Verifica intervalos de valores\n",
    "        print(f\"Intervalos de valores:\")\n",
    "        for col in df.columns:\n",
    "            min_val, max_val = df[col].min(), df[col].max()\n",
    "            print(f\"   {col}: {min_val} a {max_val}\")\n",
    "        \n",
    "        # Verifica valores faltantes\n",
    "        missing = df.isnull().sum().sum()\n",
    "        if missing == 0:\n",
    "            print(f\"Sem valores faltantes\")\n",
    "        else:\n",
    "            print(f\"Valores faltantes encontrados: {missing}\")\n",
    "        \n",
    "        # Verifica valores das semanas\n",
    "        unique_weeks = sorted(df['semana'].unique())\n",
    "        expected_weeks = [1, 2, 3, 4, 5]\n",
    "        if list(unique_weeks) == expected_weeks:\n",
    "            print(f\"Semanas corretas: {unique_weeks}\")\n",
    "        else:\n",
    "            print(f\"Semanas incorretas: {unique_weeks}\")\n",
    "            print(f\"   Esperado: {expected_weeks}\")\n",
    "        \n",
    "        # Verifica quantidades negativas\n",
    "        negative_qty = (df['quantidade'] < 0).sum()\n",
    "        if negative_qty == 0:\n",
    "            print(f\"Sem quantidades negativas\")\n",
    "        else:\n",
    "            print(f\"Quantidades negativas encontradas: {negative_qty}\")\n",
    "        \n",
    "        print(f\"\\nESTATÍSTICAS RESUMIDAS:\")\n",
    "        print(f\"   Registros: {len(df):,}\")\n",
    "        print(f\"   PDVs únicos: {df['pdv'].nunique():,}\")\n",
    "        print(f\"   Produtos únicos: {df['produto'].nunique():,}\")\n",
    "        print(f\"   Quantidade total: {df['quantidade'].sum():,}\")\n",
    "        print(f\"   Quantidade média: {df['quantidade'].mean():.1f}\")\n",
    "        \n",
    "        print(f\"\\nVALIDAÇÃO DO FORMATO HACKATHON: PASSOU ✓\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Validação falhou: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script de Execução Principal\n",
    "Defina os caminhos dos seus arquivos aqui e rode tudo. O script verifica se os arquivos existem, executa todo o pipeline, e no final valida se está tudo certinho para submissão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos de dados encontrados. Executando pipeline do hackathon...\n",
      "PREVISÃO DE VENDAS - HACKATHON\n",
      "PIPELINE DE PREVISÃO DE VENDAS - HACKATHON\n",
      "============================================================\n",
      "Inicializando DuckDB para previsão do hackathon...\n",
      "   Memória: 12GB, Threads: 10\n",
      "Carregando e juntando conjuntos de dados...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756d2b9680dc46ea8e986341e82f4b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Registros: 6,427,111\n",
      "   PDVs: 15,059\n",
      "   Produtos: 6,927\n",
      "   Combinações PDV-Produto: 1,022,226\n",
      "   Período: 2022-01-01 a 2022-12-31\n",
      "Criando agregação semanal...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1cc092d5be54a33a4b0fafcdaba1f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Registros semanais: 6,164,441\n",
      "   Intervalo de semanas: 1-52\n",
      "   Quantidade total: 53,523,122\n",
      "Criando características de ML...\n",
      "   Engenharia de características completa\n",
      "Preparando dados de treinamento...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9fb5cfca9b42d9936700026c0b4dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extraídos 6,164,441 registros\n",
      "   Características selecionadas: 26\n",
      "   Registros limpos: 6,164,441\n",
      "Criando divisão de validação temporal...\n",
      "   Treinamento: 5,685,519 registros (semanas 1-48)\n",
      "   Validação: 478,922 registros (semanas 49-52)\n",
      "Treinando modelo LightGBM...\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.509117 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3420\n",
      "[LightGBM] [Info] Number of data points in the train set: 5685519, number of used features: 26\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Start training from score 2.193065\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's poisson: -7.606\n",
      "[200]\tvalid_0's poisson: -7.54448\n",
      "Early stopping, best iteration is:\n",
      "[158]\tvalid_0's poisson: -7.6997\n",
      "   Treinamento concluído em 73.6s\n",
      "Validando modelo...\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "   WMAPE = 0.4629 (46.29%)\n",
      "   MAE = 2.48\n",
      "Retreinando com conjunto completo de dados de 2022...\n",
      "Treinando modelo LightGBM...\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.392981 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3541\n",
      "[LightGBM] [Info] Number of data points in the train set: 6164441, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score 2.161316\n",
      "   Treinamento concluído em 254.3s\n",
      "Gerando previsões para Janeiro 2023 (semanas 1-5)...\n",
      "   Extraindo estados mais recentes para todas as combinações PDV-Produto...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae9e7e87d7141ecb95f0cec567e0ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Encontradas 1,022,226 combinações únicas PDV-Produto\n",
      "   Criando características de previsão para semanas [1, 2, 3, 4, 5]...\n",
      "   Preparando 5,111,130 previsões...\n",
      "   Processando lote 1/3 (500,000 combinações)\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "   Progresso: 33.3% (2,500,000 previsões geradas)\n",
      "   Processando lote 2/3 (500,000 combinações)\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "   Progresso: 66.7% (5,000,000 previsões geradas)\n",
      "   Processando lote 3/3 (22,226 combinações)\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "   Progresso: 100.0% (5,111,130 previsões geradas)\n",
      "   Geradas 5,111,130 previsões\n",
      "   PDVs únicos: 15,059\n",
      "   Produtos únicos: 6,927\n",
      "   Quantidade média: 6.5\n",
      "   Resumo semanal:\n",
      "      Semana 1: 1,022,226 previsões, 6,583,985 qtd total, 6.4 qtd média\n",
      "      Semana 2: 1,022,226 previsões, 6,656,842 qtd total, 6.5 qtd média\n",
      "      Semana 3: 1,022,226 previsões, 6,642,560 qtd total, 6.5 qtd média\n",
      "      Semana 4: 1,022,226 previsões, 6,656,805 qtd total, 6.5 qtd média\n",
      "      Semana 5: 1,022,226 previsões, 6,693,750 qtd total, 6.5 qtd média\n",
      "Salvando previsão no formato do hackathon...\n",
      "   Salvo em: hackathon_v1.csv\n",
      "   Formato: CSV com separador ';', codificação UTF-8\n",
      "   Registros: 5,111,130\n",
      "============================================================\n",
      "Tempo total: 1586.0s (26.4min)\n",
      "Previsões: 5,111,130\n",
      "Saída: hackathon_v1.csv\n",
      "\n",
      "RESULTADOS DA VALIDAÇÃO (semanas 49-52 de 2022):\n",
      "   LightGBM: 46.29% WMAPE\n",
      "   MAE: 2.48\n",
      "\n",
      "SUBMISSÃO DO HACKATHON PRONTA!\n",
      "==================================================\n",
      "Arquivo: hackathon_v1.csv\n",
      "Formato: semana;pdv;produto;quantidade\n",
      "Codificação: UTF-8\n",
      "Semanas: 1-5 (Janeiro 2023)\n",
      "\n",
      "AMOSTRA DAS PREVISÕES:\n",
      " semana                 pdv             produto  quantidade\n",
      "      1 1386320779860921194 5142193029158980052           1\n",
      "      1 1386320779860921194 7274081052031128506           7\n",
      "      1 1386320779860921194 8907304283547263021           2\n",
      "      1 1386320779860921194 9036639434778896273           2\n",
      "      1 1387963652884512110 2729570553257656610           6\n",
      "      1 1387963652884512110 2811937796518769916           6\n",
      "      1 1387963652884512110 4397586149099996247           1\n",
      "      1 1387963652884512110 4855166111875093266           6\n",
      "      1 1388825105030488269 3957423958447917354           3\n",
      "      1 1388825105030488269 4051297824522998476          14\n",
      "\n",
      "RESUMO SEMANAL:\n",
      "        Previsões  Qtd_Total  Qtd_Média\n",
      "semana                                 \n",
      "1         1022226    6583985        6.4\n",
      "2         1022226    6656842        6.5\n",
      "3         1022226    6642560        6.5\n",
      "4         1022226    6656805        6.5\n",
      "5         1022226    6693750        6.5\n",
      "\n",
      "VERIFICAÇÕES DE QUALIDADE:\n",
      "   Total de previsões: 5,111,130\n",
      "   PDVs únicos: 15,059\n",
      "   Produtos únicos: 6,927\n",
      "   Semanas cobertas: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5)]\n",
      "   Tipos de dados: Todos inteiros = True\n",
      "   Valores faltantes: 0\n",
      "   Quantidades negativas: 0\n",
      "   Tamanho do arquivo: 214.2 MB\n",
      "\n",
      "Pronto para submissão no hackathon!\n",
      "\n",
      "============================================================\n",
      "Arquivo carregado com sucesso\n",
      "Forma: (5111130, 4)\n",
      "Nomes das colunas corretos: ['semana', 'pdv', 'produto', 'quantidade']\n",
      "Tipos de dados:\n",
      "   ✓ semana: int64\n",
      "   ✓ pdv: int64\n",
      "      ID grande detectado (máx: 9223357287569232769) - usando 64-bit\n",
      "   ✓ produto: int64\n",
      "      ID grande detectado (máx: 9221123418603951768) - usando 64-bit\n",
      "   ✓ quantidade: int64\n",
      "Intervalos de valores:\n",
      "   semana: 1 a 5\n",
      "   pdv: 163867944404228 a 9223357287569232769\n",
      "   produto: 4264882989326697 a 9221123418603951768\n",
      "   quantidade: 0 a 24086\n",
      "Sem valores faltantes\n",
      "Semanas corretas: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5)]\n",
      "Sem quantidades negativas\n",
      "\n",
      "ESTATÍSTICAS RESUMIDAS:\n",
      "   Registros: 5,111,130\n",
      "   PDVs únicos: 15,059\n",
      "   Produtos únicos: 6,927\n",
      "   Quantidade total: 33,233,942\n",
      "   Quantidade média: 6.5\n",
      "\n",
      "VALIDAÇÃO DO FORMATO HACKATHON: PASSOU ✓\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Caminhos dos arquivos - AJUSTAR ESTES PARA SEUS ARQUIVOS DE DADOS\n",
    "    arquivo_loja = \"../data/raw/file1.parquet\"        # Dados da loja\n",
    "    arquivo_transacao = \"../data/raw/file2.parquet\"   # Dados de transação  \n",
    "    arquivo_produto = \"../data/raw/file3.parquet\"     # Dados do produto\n",
    "    arquivo_saida = \"hackathon_v1.csv\"\n",
    "    \n",
    "    # Verifica se os arquivos existem\n",
    "    arquivos_existem = all(os.path.exists(f) for f in [arquivo_loja, arquivo_transacao, arquivo_produto])\n",
    "    \n",
    "    if arquivos_existem:\n",
    "        print(\"Arquivos de dados encontrados. Executando pipeline do hackathon...\")\n",
    "        \n",
    "        # Executa o pipeline de previsão\n",
    "        forecast_result, validation_metrics = executar_previsao_hackathon(\n",
    "            arquivo_loja=arquivo_loja,\n",
    "            arquivo_transacao=arquivo_transacao, \n",
    "            arquivo_produto=arquivo_produto,\n",
    "            arquivo_saida=arquivo_saida\n",
    "        )\n",
    "        \n",
    "        if forecast_result is not None:\n",
    "            # Valida a saída\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            validar_saida_hackathon(arquivo_saida)\n",
    "            \n",
    "        else:\n",
    "            print(\"Previsão falhou!\")\n",
    "    else:\n",
    "        print(\"Arquivos de dados não encontrados. Verifique os caminhos dos arquivos:\")\n",
    "        for f in [arquivo_loja, arquivo_transacao, arquivo_produto]:\n",
    "            exists = \"✓\" if os.path.exists(f) else \"✗\"\n",
    "            print(f\"   {exists} {f}\")\n",
    "        print(\"\\nAjuste os caminhos dos arquivos no script e tente novamente.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
